{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport timm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport random\n\n# Set the device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T07:50:40.047963Z","iopub.execute_input":"2025-02-27T07:50:40.048280Z","iopub.status.idle":"2025-02-27T07:50:40.054447Z","shell.execute_reply.started":"2025-02-27T07:50:40.048259Z","shell.execute_reply":"2025-02-27T07:50:40.053561Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Define transforms for training and testing\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Define directory paths (adjust if needed)\ntrain_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray/train\"\nval_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray/val\"\ntest_dir = \"/kaggle/input/chest-xray-pneumonia/chest_xray/test\"\n\n# Create datasets using ImageFolder\ntrain_dataset = datasets.ImageFolder(root=train_dir, transform=train_transform)\nval_dataset = datasets.ImageFolder(root=val_dir, transform=test_transform)\ntest_dataset = datasets.ImageFolder(root=test_dir, transform=test_transform)\n\nprint(\"Train samples:\", len(train_dataset))\nprint(\"Validation samples:\", len(val_dataset))\nprint(\"Test samples:\", len(test_dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T07:50:49.026967Z","iopub.execute_input":"2025-02-27T07:50:49.027284Z","iopub.status.idle":"2025-02-27T07:50:51.077367Z","shell.execute_reply.started":"2025-02-27T07:50:49.027258Z","shell.execute_reply":"2025-02-27T07:50:51.076605Z"}},"outputs":[{"name":"stdout","text":"Train samples: 5216\nValidation samples: 16\nTest samples: 624\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"batch_size = 32\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\n# Get class names (e.g., ['NORMAL', 'PNEUMONIA'])\nclass_names = train_dataset.classes\nprint(\"Classes:\", class_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T07:50:55.346640Z","iopub.execute_input":"2025-02-27T07:50:55.346975Z","iopub.status.idle":"2025-02-27T07:50:55.352514Z","shell.execute_reply.started":"2025-02-27T07:50:55.346948Z","shell.execute_reply":"2025-02-27T07:50:55.351467Z"}},"outputs":[{"name":"stdout","text":"Classes: ['NORMAL', 'PNEUMONIA']\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# Create the EfficientNet-B0 model with pretrained weights\nmodel = timm.create_model('efficientnet_b0', pretrained=True)\n\n# For EfficientNet from timm, the final classifier layer is 'classifier'\nnum_ftrs = model.classifier.in_features\nmodel.classifier = nn.Linear(num_ftrs, 2)\n\nmodel = model.to(device)\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T07:51:03.032392Z","iopub.execute_input":"2025-02-27T07:51:03.032690Z","iopub.status.idle":"2025-02-27T07:51:03.327429Z","shell.execute_reply.started":"2025-02-27T07:51:03.032668Z","shell.execute_reply":"2025-02-27T07:51:03.326675Z"}},"outputs":[{"name":"stdout","text":"EfficientNet(\n  (conv_stem): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n  (bn1): BatchNormAct2d(\n    32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n    (drop): Identity()\n    (act): SiLU(inplace=True)\n  )\n  (blocks): Sequential(\n    (0): Sequential(\n      (0): DepthwiseSeparableConv(\n        (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n        (bn1): BatchNormAct2d(\n          32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn2): BatchNormAct2d(\n          16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n    (1): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n        (bn2): BatchNormAct2d(\n          96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (1): InvertedResidual(\n        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n        (bn2): BatchNormAct2d(\n          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n    (2): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n        (bn2): BatchNormAct2d(\n          144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (1): InvertedResidual(\n        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n        (bn2): BatchNormAct2d(\n          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n    (3): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n        (bn2): BatchNormAct2d(\n          240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (1): InvertedResidual(\n        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n        (bn2): BatchNormAct2d(\n          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (2): InvertedResidual(\n        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n        (bn2): BatchNormAct2d(\n          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n    (4): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n        (bn2): BatchNormAct2d(\n          480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (1): InvertedResidual(\n        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n        (bn2): BatchNormAct2d(\n          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (2): InvertedResidual(\n        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n        (bn2): BatchNormAct2d(\n          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n    (5): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n        (bn2): BatchNormAct2d(\n          672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (1): InvertedResidual(\n        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n        (bn2): BatchNormAct2d(\n          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (2): InvertedResidual(\n        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n        (bn2): BatchNormAct2d(\n          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n      (3): InvertedResidual(\n        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n        (bn2): BatchNormAct2d(\n          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n    (6): Sequential(\n      (0): InvertedResidual(\n        (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNormAct2d(\n          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n        (bn2): BatchNormAct2d(\n          1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): SiLU(inplace=True)\n        )\n        (aa): Identity()\n        (se): SqueezeExcite(\n          (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n          (act1): SiLU(inplace=True)\n          (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n          (gate): Sigmoid()\n        )\n        (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNormAct2d(\n          320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n          (drop): Identity()\n          (act): Identity()\n        )\n        (drop_path): Identity()\n      )\n    )\n  )\n  (conv_head): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn2): BatchNormAct2d(\n    1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n    (drop): Identity()\n    (act): SiLU(inplace=True)\n  )\n  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n  (classifier): Linear(in_features=1280, out_features=2, bias=True)\n)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def train_one_epoch(model, loader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    running_corrects = 0\n    total = 0\n    for inputs, labels in loader:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.size(0)\n        _, preds = torch.max(outputs, 1)\n        running_corrects += torch.sum(preds == labels.data)\n        total += inputs.size(0)\n    epoch_loss = running_loss / total\n    epoch_acc = running_corrects.double() / total\n    return epoch_loss, epoch_acc.item()\n\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    running_corrects = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n            _, preds = torch.max(outputs, 1)\n            running_corrects += torch.sum(preds == labels.data)\n            total += inputs.size(0)\n    epoch_loss = running_loss / total\n    epoch_acc = running_corrects.double() / total\n    return epoch_loss, epoch_acc.item()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\nnum_epochs = 30\nfor epoch in range(1, num_epochs+1):\n    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n    test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n    print(f\"Epoch {epoch}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T07:51:11.248190Z","iopub.execute_input":"2025-02-27T07:51:11.248489Z","iopub.status.idle":"2025-02-27T08:17:13.444686Z","shell.execute_reply.started":"2025-02-27T07:51:11.248466Z","shell.execute_reply":"2025-02-27T08:17:13.443758Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30 - Train Loss: 0.1368, Train Acc: 0.9586 | Test Loss: 0.8082, Test Acc: 0.7404\nEpoch 2/30 - Train Loss: 0.0363, Train Acc: 0.9887 | Test Loss: 0.4392, Test Acc: 0.8862\nEpoch 3/30 - Train Loss: 0.0187, Train Acc: 0.9952 | Test Loss: 0.6130, Test Acc: 0.8542\nEpoch 4/30 - Train Loss: 0.0136, Train Acc: 0.9965 | Test Loss: 0.9294, Test Acc: 0.7965\nEpoch 5/30 - Train Loss: 0.0100, Train Acc: 0.9965 | Test Loss: 0.9842, Test Acc: 0.8221\nEpoch 6/30 - Train Loss: 0.0064, Train Acc: 0.9985 | Test Loss: 0.9937, Test Acc: 0.8301\nEpoch 7/30 - Train Loss: 0.0063, Train Acc: 0.9983 | Test Loss: 1.3272, Test Acc: 0.7853\nEpoch 8/30 - Train Loss: 0.0109, Train Acc: 0.9960 | Test Loss: 1.5981, Test Acc: 0.7308\nEpoch 9/30 - Train Loss: 0.0045, Train Acc: 0.9988 | Test Loss: 1.0676, Test Acc: 0.8317\nEpoch 10/30 - Train Loss: 0.0028, Train Acc: 0.9990 | Test Loss: 1.4881, Test Acc: 0.7772\nEpoch 11/30 - Train Loss: 0.0009, Train Acc: 1.0000 | Test Loss: 1.4355, Test Acc: 0.7788\nEpoch 12/30 - Train Loss: 0.0029, Train Acc: 0.9992 | Test Loss: 1.7235, Test Acc: 0.7772\nEpoch 13/30 - Train Loss: 0.0091, Train Acc: 0.9971 | Test Loss: 1.4668, Test Acc: 0.7788\nEpoch 14/30 - Train Loss: 0.0033, Train Acc: 0.9992 | Test Loss: 1.4290, Test Acc: 0.7917\nEpoch 15/30 - Train Loss: 0.0035, Train Acc: 0.9990 | Test Loss: 0.8286, Test Acc: 0.8510\nEpoch 16/30 - Train Loss: 0.0032, Train Acc: 0.9985 | Test Loss: 1.7719, Test Acc: 0.7420\nEpoch 17/30 - Train Loss: 0.0013, Train Acc: 0.9996 | Test Loss: 1.5863, Test Acc: 0.7804\nEpoch 18/30 - Train Loss: 0.0054, Train Acc: 0.9985 | Test Loss: 1.9232, Test Acc: 0.7436\nEpoch 19/30 - Train Loss: 0.0033, Train Acc: 0.9987 | Test Loss: 1.8715, Test Acc: 0.7644\nEpoch 20/30 - Train Loss: 0.0017, Train Acc: 0.9992 | Test Loss: 1.5934, Test Acc: 0.7756\nEpoch 21/30 - Train Loss: 0.0026, Train Acc: 0.9990 | Test Loss: 0.9421, Test Acc: 0.8494\nEpoch 22/30 - Train Loss: 0.0017, Train Acc: 0.9996 | Test Loss: 1.7188, Test Acc: 0.7788\nEpoch 23/30 - Train Loss: 0.0006, Train Acc: 0.9998 | Test Loss: 2.5686, Test Acc: 0.7276\nEpoch 24/30 - Train Loss: 0.0010, Train Acc: 0.9996 | Test Loss: 1.6760, Test Acc: 0.7901\nEpoch 25/30 - Train Loss: 0.0002, Train Acc: 1.0000 | Test Loss: 1.6964, Test Acc: 0.7901\nEpoch 26/30 - Train Loss: 0.0029, Train Acc: 0.9992 | Test Loss: 1.3530, Test Acc: 0.8317\nEpoch 27/30 - Train Loss: 0.0029, Train Acc: 0.9990 | Test Loss: 0.5889, Test Acc: 0.9151\nEpoch 28/30 - Train Loss: 0.0025, Train Acc: 0.9992 | Test Loss: 1.3394, Test Acc: 0.8381\nEpoch 29/30 - Train Loss: 0.0003, Train Acc: 1.0000 | Test Loss: 1.7054, Test Acc: 0.8013\nEpoch 30/30 - Train Loss: 0.0002, Train Acc: 1.0000 | Test Loss: 1.4301, Test Acc: 0.8221\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"torch.save(model.state_dict(), \"best_model.pth\")\nprint(\"Model saved.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T08:17:13.446420Z","iopub.execute_input":"2025-02-27T08:17:13.446788Z","iopub.status.idle":"2025-02-27T08:17:13.516359Z","shell.execute_reply.started":"2025-02-27T08:17:13.446755Z","shell.execute_reply":"2025-02-27T08:17:13.515468Z"}},"outputs":[{"name":"stdout","text":"Model saved.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Cell: GradCAM implementation for EfficientNet-B0\n\nclass GradCAM:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n        self.hook_handles = []\n        self._register_hooks()\n        \n    def _register_hooks(self):\n        def forward_hook(module, input, output):\n            self.activations = output.detach()\n        \n        def backward_hook(module, grad_in, grad_out):\n            self.gradients = grad_out[0].detach()\n        \n        handle_forward = self.target_layer.register_forward_hook(forward_hook)\n        handle_backward = self.target_layer.register_backward_hook(backward_hook)\n        self.hook_handles.extend([handle_forward, handle_backward])\n    \n    def generate_cam(self, input_tensor, class_idx=None):\n        self.model.zero_grad()\n        output = self.model(input_tensor)\n        if class_idx is None:\n            class_idx = output.argmax(dim=1).item()\n        score = output[0, class_idx]\n        score.backward()\n        \n        # Global average pooling of gradients\n        gradients = self.gradients  # shape: [1, C, H, W]\n        activations = self.activations  # shape: [1, C, H, W]\n        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)  # shape: [1, C, 1, 1]\n        cam = torch.sum(weights * activations, dim=1)  # shape: [1, H, W]\n        cam = torch.relu(cam)\n        cam = cam - cam.min()\n        cam = cam / (cam.max() + 1e-8)\n        cam = cam.cpu().numpy()[0]\n        cam = cv2.resize(cam, (224, 224))\n        return cam\n\n    def remove_hooks(self):\n        for handle in self.hook_handles:\n            handle.remove()\n\n\ndef show_gradcam(model, img_tensor, true_label, class_names):\n    model.eval()\n    # For EfficientNet-B0, target the last convolutional layer via 'conv_head'\n    target_layer = model.conv_head\n    grad_cam = GradCAM(model, target_layer)\n    \n    input_tensor = img_tensor.unsqueeze(0).to(device)\n    cam = grad_cam.generate_cam(input_tensor)\n    \n    # Denormalize the image for visualization (using ImageNet normalization)\n    inv_normalize = transforms.Normalize(\n        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n        std=[1/0.229, 1/0.224, 1/0.225]\n    )\n    img_denorm = inv_normalize(img_tensor).cpu().numpy().transpose((1,2,0))\n    img_denorm = np.clip(img_denorm, 0, 1)\n    \n    # Create a heatmap and overlay it on the original image\n    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n    heatmap = np.float32(heatmap) / 255\n    overlay = heatmap + np.float32(img_denorm)\n    overlay = overlay / np.max(overlay)\n    \n    # Get the model's prediction\n    output = model(input_tensor)\n    _, pred = torch.max(output, 1)\n    pred_label = class_names[pred.item()]\n    true_label_str = class_names[true_label]\n    \n    # Plot side-by-side the original image and the GradCAM overlay\n    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n    axs[0].imshow(img_denorm)\n    axs[0].set_title(f\"Original\\nTrue: {true_label_str}\")\n    axs[0].axis('off')\n    axs[1].imshow(overlay)\n    axs[1].set_title(f\"GradCAM Overlay\\nPredicted: {pred_label}\")\n    axs[1].axis('off')\n    plt.show()\n    \n    grad_cam.remove_hooks()\n\n# Example usage:\n# Ensure you have a test DataLoader named 'test_loader' and the class_names list (e.g., ['NORMAL', 'PNEUMONIA'])\ninputs, labels = next(iter(test_loader))\nidx = random.randint(0, inputs.size(0) - 1)\nshow_gradcam(model, inputs[idx], labels[idx].item(), class_names)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}